{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_pytorch_computer_vision_exercises.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Azewkhan/Azewkhan/blob/main/extras/exercises/03_pytorch_computer_vision_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03. PyTorch Computer Vision Exercises\n",
        "\n",
        "The following is a collection of exercises based on computer vision fundamentals in PyTorch.\n",
        "\n",
        "They're a bunch of fun.\n",
        "\n",
        "You're going to get to write plenty of code!\n",
        "\n",
        "## Resources\n",
        "\n",
        "1. These exercises are based on [notebook 03 of the Learn PyTorch for Deep Learning course](https://www.learnpytorch.io/03_pytorch_computer_vision/).\n",
        "2. See a live [walkthrough of the solutions (errors and all) on YouTube](https://youtu.be/_PibmqpEyhA).\n",
        "  * **Note:** Going through these exercises took me just over 3 hours of solid coding, so you should expect around the same.\n",
        "3. See [other solutions on the course GitHub](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions)."
      ],
      "metadata": {
        "id": "Vex99np2wFVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaeYzOTLwWh2",
        "outputId": "4d7d4458-9758-45ca-a5c3-f7eec402f3f9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Mar 27 22:47:24 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import torch\n",
        "import torch\n",
        "\n",
        "# Exercises require PyTorch > 1.10.0\n",
        "print(torch.__version__)\n",
        "\n",
        "# TODO: Setup device agnostic code\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNwZLMbCzJLk",
        "outputId": "3bc1d401-bd35-4e4b-9145-a9dd76478cb7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. What are 3 areas in industry where computer vision is currently being used?"
      ],
      "metadata": {
        "id": "FSFX7tc1w-en"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manufacturing: a computer vision system can be used to inspect products for defects, such as scratches or dents.\n",
        "Healthcare: Computer vision can help doctors to identify tumors in medical images\n",
        "Retail: CComputer vision can be used to track inventory levels in a store and identify when items need to be restocked"
      ],
      "metadata": {
        "id": "dNGAmqJuztah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Search \"what is overfitting in machine learning\" and write down a sentence about what you find."
      ],
      "metadata": {
        "id": "oBK-WI6YxDYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting happens when a machine learning model learns the training data too well, performing great on training data but poorly on unseen data due to memorization instead of generalization."
      ],
      "metadata": {
        "id": "wQwxa68W0Ik4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Search \"ways to prevent overfitting in machine learning\", write down 3 of the things you find and a sentence about each.\n",
        "> **Note:** there are lots of these, so don't worry too much about all of them, just pick 3 and start with those."
      ],
      "metadata": {
        "id": "XeYFEqw8xK26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 ways to prevent overfitting in machine learning, along with a sentence about each:\n",
        "\n",
        "Data augmentation: Artificially increasing the size of the training dataset by creating modified versions of existing data. By adding more diverse data, the model is less likely to memorize the original training set.\n",
        "\n",
        "Regularization: Adding penalty terms to the loss function during training that discourages overly complex models. This helps to prevent the model from fitting the noise in the data.\n",
        "\n",
        "Early stopping: Monitoring the model's performance on a validation set during training and stopping the training process when the performance starts to decrease. This helps to prevent the model from overfitting to the training data."
      ],
      "metadata": {
        "id": "77SuQV490Ug5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Spend 20-minutes reading and clicking through the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/).\n",
        "\n",
        "* Upload your own example image using the \"upload\" button on the website and see what happens in each layer of a CNN as your image passes through it."
      ],
      "metadata": {
        "id": "DKdEEFEqxM-8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TqZaJIRMbFtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Load the [`torchvision.datasets.MNIST()`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) train and test datasets."
      ],
      "metadata": {
        "id": "lvf-3pODxXYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the transform to convert images to tensors and normalize\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "# Load the training dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
        "                                           train=True,\n",
        "                                           download=True,\n",
        "                                           transform=transform)\n",
        "\n",
        "# Load the testing dataset\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data',\n",
        "                                          train=False,\n",
        "                                          download=True,\n",
        "                                          transform=transform)"
      ],
      "metadata": {
        "id": "SHjeuN81bHza",
        "outputId": "eafee06c-8daa-4d77-877b-cdea5eb6784a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 15.9MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 474kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.81MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.30MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Visualize at least 5 different samples of the MNIST training dataset."
      ],
      "metadata": {
        "id": "qxZW-uAbxe_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get 5 random samples from the training dataset\n",
        "indices = np.random.choice(len(train_dataset), size=5, replace=False)\n",
        "samples = [train_dataset[i] for i in indices]\n",
        "\n",
        "# Create a figure and axes\n",
        "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
        "\n",
        "# Plot the samples\n",
        "for i, (image, label) in enumerate(samples):\n",
        "    axes[i].imshow(image.squeeze(), cmap='gray')  # Squeeze to remove channel dimension\n",
        "    axes[i].set_title(f\"Label: {label}\")\n",
        "    axes[i].axis('off')  # Hide axes\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QVFsYi1PbItE",
        "outputId": "48bd118d-9777-465d-f298-be8be156649b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x300 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Turn the MNIST train and test datasets into dataloaders using `torch.utils.data.DataLoader`, set the `batch_size=32`."
      ],
      "metadata": {
        "id": "JAPDzW0wxhi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create the train dataloader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Create the test dataloader\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "ALA6MPcFbJXQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Recreate `model_2` used in notebook 03 (the same model from the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/), also known as TinyVGG) capable of fitting on the MNIST dataset."
      ],
      "metadata": {
        "id": "bCCVfXk5xjYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class TinyVGG(nn.Module):\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape,\n",
        "                     out_channels=hidden_units,\n",
        "                     kernel_size=3,\n",
        "                     stride=1,\n",
        "                     padding=1), # values for kernel_size, stride and padding are from CNN explainer website\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                     out_channels=hidden_units,\n",
        "                     kernel_size=3,\n",
        "                     stride=1,\n",
        "                     padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2) # default stride value is kernel_size\n",
        "        )\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=hidden_units*7*7,\n",
        "                      out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block_1(x)\n",
        "        x = self.conv_block_2(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the model with appropriate input/output shapes\n",
        "model_2 = TinyVGG(input_shape=1, hidden_units=10, output_shape=10)"
      ],
      "metadata": {
        "id": "5IKNF22XbKYS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Train the model you built in exercise 8. for 5 epochs on CPU and GPU and see how long it takes on each."
      ],
      "metadata": {
        "id": "sf_3zUr7xlhy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSo6vVWFbNLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Make predictions using your trained model and visualize at least 5 of them comparing the prediciton to the target label."
      ],
      "metadata": {
        "id": "w1CsHhPpxp1w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_YGgZvSobNxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Plot a confusion matrix comparing your model's predictions to the truth labels."
      ],
      "metadata": {
        "id": "qQwzqlBWxrpG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vSrXiT_AbQ6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Create a random tensor of shape `[1, 3, 64, 64]` and pass it through a `nn.Conv2d()` layer with various hyperparameter settings (these can be any settings you choose), what do you notice if the `kernel_size` parameter goes up and down?"
      ],
      "metadata": {
        "id": "lj6bDhoWxt2y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "leCTsqtSbR5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. Use a model similar to the trained `model_2` from notebook 03 to make predictions on the test [`torchvision.datasets.FashionMNIST`](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html) dataset.\n",
        "* Then plot some predictions where the model was wrong alongside what the label of the image should've been.\n",
        "* After visualing these predictions do you think it's more of a modelling error or a data error?\n",
        "* As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?"
      ],
      "metadata": {
        "id": "VHS20cNTxwSi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78a8LjtdbSZj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}